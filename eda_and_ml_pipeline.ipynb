{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Factuality Detection in AI-Generated Educational Content\n",
        "\n",
        "## Data4Good Competition - 4th Annual\n",
        "\n",
        "This notebook covers:\n",
        "1. Exploratory Data Analysis (EDA)\n",
        "2. Feature Engineering\n",
        "3. Machine Learning Model Development\n",
        "4. Model Evaluation and Comparison\n",
        "5. Test Set Predictions\n",
        "6. Methodology Discussion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# For reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Initial Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "with open('data/train.json', 'r', encoding='utf-8') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Load test data\n",
        "with open('data/test.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Convert to DataFrames\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
        "print(\"Test data columns:\", test_df.columns.tolist())\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Training Data Info:\")\n",
        "print(train_df.info())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\nTest Data Info:\")\n",
        "print(test_df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values in training data:\")\n",
        "print(train_df.isnull().sum())\n",
        "print(\"\\nMissing values in test data:\")\n",
        "print(test_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target distribution\n",
        "print(\"Target variable distribution:\")\n",
        "print(train_df['type'].value_counts())\n",
        "print(\"\\nTarget variable percentages:\")\n",
        "print(train_df['type'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualize target distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "train_df['type'].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c', '#f39c12'])\n",
        "axes[0].set_title('Distribution of Answer Types', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Type', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Pie chart\n",
        "train_df['type'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
        "                                      colors=['#2ecc71', '#e74c3c', '#f39c12'])\n",
        "axes[1].set_title('Percentage Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text length analysis\n",
        "train_df['question_length'] = train_df['question'].str.len()\n",
        "train_df['context_length'] = train_df['context'].str.len()\n",
        "train_df['answer_length'] = train_df['answer'].str.len()\n",
        "\n",
        "# Word count analysis\n",
        "train_df['question_words'] = train_df['question'].str.split().str.len()\n",
        "train_df['context_words'] = train_df['context'].str.split().str.len()\n",
        "train_df['answer_words'] = train_df['answer'].str.split().str.len()\n",
        "\n",
        "# Display statistics\n",
        "print(\"Text Length Statistics:\")\n",
        "print(train_df[['question_length', 'context_length', 'answer_length', \n",
        "                'question_words', 'context_words', 'answer_words']].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize text length distributions by type\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Question length by type\n",
        "for i, col in enumerate(['question_length', 'context_length', 'answer_length']):\n",
        "    for type_val in train_df['type'].unique():\n",
        "        data = train_df[train_df['type'] == type_val][col]\n",
        "        axes[0, i].hist(data, alpha=0.6, label=type_val, bins=50)\n",
        "    axes[0, i].set_title(f'{col.replace(\"_\", \" \").title()} Distribution', fontweight='bold')\n",
        "    axes[0, i].set_xlabel('Length (characters)')\n",
        "    axes[0, i].set_ylabel('Frequency')\n",
        "    axes[0, i].legend()\n",
        "    axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "# Word count by type\n",
        "for i, col in enumerate(['question_words', 'context_words', 'answer_words']):\n",
        "    for type_val in train_df['type'].unique():\n",
        "        data = train_df[train_df['type'] == type_val][col]\n",
        "        axes[1, i].hist(data, alpha=0.6, label=type_val, bins=50)\n",
        "    axes[1, i].set_title(f'{col.replace(\"_\", \" \").title()} Distribution', fontweight='bold')\n",
        "    axes[1, i].set_xlabel('Word Count')\n",
        "    axes[1, i].set_ylabel('Frequency')\n",
        "    axes[1, i].legend()\n",
        "    axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots for text lengths by type\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, col in enumerate(['question_length', 'context_length', 'answer_length']):\n",
        "    train_df.boxplot(column=col, by='type', ax=axes[i])\n",
        "    axes[i].set_title(f'{col.replace(\"_\", \" \").title()} by Type', fontweight='bold')\n",
        "    axes[i].set_xlabel('Type')\n",
        "    axes[i].set_ylabel('Length (characters)')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample examples from each class\n",
        "print(\"=\"*80)\n",
        "print(\"SAMPLE EXAMPLES FROM EACH CLASS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for type_val in train_df['type'].unique():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TYPE: {type_val.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    sample = train_df[train_df['type'] == type_val].iloc[0]\n",
        "    print(f\"\\nQuestion: {sample['question']}\")\n",
        "    print(f\"\\nContext: {sample['context'][:200]}...\" if len(sample['context']) > 200 else f\"\\nContext: {sample['context']}\")\n",
        "    print(f\"\\nAnswer: {sample['answer']}\")\n",
        "    print(f\"\\nType: {sample['type']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features(df):\n",
        "    \"\"\"\n",
        "    Extract various features from question, context, and answer\n",
        "    \"\"\"\n",
        "    features = df.copy()\n",
        "    \n",
        "    # Basic text features\n",
        "    features['question_length'] = features['question'].str.len()\n",
        "    features['context_length'] = features['context'].str.len()\n",
        "    features['answer_length'] = features['answer'].str.len()\n",
        "    \n",
        "    features['question_words'] = features['question'].str.split().str.len()\n",
        "    features['context_words'] = features['context'].str.split().str.len()\n",
        "    features['answer_words'] = features['answer'].str.split().str.len()\n",
        "    \n",
        "    # Ratio features\n",
        "    features['answer_to_question_ratio'] = features['answer_length'] / (features['question_length'] + 1)\n",
        "    features['answer_to_context_ratio'] = features['answer_length'] / (features['context_length'] + 1)\n",
        "    features['question_to_context_ratio'] = features['question_length'] / (features['context_length'] + 1)\n",
        "    \n",
        "    # Word overlap features\n",
        "    def word_overlap(text1, text2):\n",
        "        words1 = set(text1.lower().split())\n",
        "        words2 = set(text2.lower().split())\n",
        "        if len(words1) == 0 or len(words2) == 0:\n",
        "            return 0\n",
        "        return len(words1.intersection(words2)) / len(words1.union(words2))\n",
        "    \n",
        "    features['question_answer_overlap'] = features.apply(\n",
        "        lambda x: word_overlap(x['question'], x['answer']), axis=1\n",
        "    )\n",
        "    features['context_answer_overlap'] = features.apply(\n",
        "        lambda x: word_overlap(x['context'], x['answer']), axis=1\n",
        "    )\n",
        "    features['question_context_overlap'] = features.apply(\n",
        "        lambda x: word_overlap(x['question'], x['context']), axis=1\n",
        "    )\n",
        "    \n",
        "    # Question word features\n",
        "    question_words = ['what', 'who', 'when', 'where', 'why', 'how', 'which', 'whom', 'whose']\n",
        "    for qw in question_words:\n",
        "        features[f'has_{qw}'] = features['question'].str.lower().str.contains(qw, regex=False).astype(int)\n",
        "    \n",
        "    # Answer starts with question word\n",
        "    features['answer_starts_with_question_word'] = features.apply(\n",
        "        lambda x: any(x['answer'].lower().startswith(qw) for qw in question_words), axis=1\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Number of sentences\n",
        "    features['question_sentences'] = features['question'].str.count(r'[.!?]+')\n",
        "    features['context_sentences'] = features['context'].str.count(r'[.!?]+')\n",
        "    features['answer_sentences'] = features['answer'].str.count(r'[.!?]+')\n",
        "    \n",
        "    # Capitalization features\n",
        "    features['answer_caps_ratio'] = features['answer'].str.findall(r'[A-Z]').str.len() / (features['answer_length'] + 1)\n",
        "    features['question_caps_ratio'] = features['question'].str.findall(r'[A-Z]').str.len() / (features['question_length'] + 1)\n",
        "    \n",
        "    # Special characters\n",
        "    features['answer_special_chars'] = features['answer'].str.findall(r'[^a-zA-Z0-9\\s]').str.len()\n",
        "    features['question_special_chars'] = features['question'].str.findall(r'[^a-zA-Z0-9\\s]').str.len()\n",
        "    \n",
        "    # Numeric features\n",
        "    features['answer_has_numbers'] = features['answer'].str.contains(r'\\d', regex=True).astype(int)\n",
        "    features['question_has_numbers'] = features['question'].str.contains(r'\\d', regex=True).astype(int)\n",
        "    features['context_has_numbers'] = features['context'].str.contains(r'\\d', regex=True).astype(int)\n",
        "    \n",
        "    return features\n",
        "\n",
        "# Apply feature engineering\n",
        "print(\"Extracting features from training data...\")\n",
        "train_features = extract_features(train_df)\n",
        "print(\"Extracting features from test data...\")\n",
        "test_features = extract_features(test_df)\n",
        "print(\"Feature engineering complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate semantic similarity using TF-IDF and cosine similarity\n",
        "print(\"Calculating semantic similarities...\")\n",
        "\n",
        "# Combine question and context for better comparison\n",
        "train_features['question_context_combined'] = train_features['question'] + ' ' + train_features['context']\n",
        "test_features['question_context_combined'] = test_features['question'] + ' ' + test_features['context']\n",
        "\n",
        "# TF-IDF vectorization for semantic similarity\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "\n",
        "# Fit on training data\n",
        "train_qc_vectors = vectorizer.fit_transform(train_features['question_context_combined'])\n",
        "train_answer_vectors = vectorizer.transform(train_features['answer'])\n",
        "\n",
        "# Transform test data\n",
        "test_qc_vectors = vectorizer.transform(test_features['question_context_combined'])\n",
        "test_answer_vectors = vectorizer.transform(test_features['answer'])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "train_features['semantic_similarity'] = [\n",
        "    cosine_similarity(train_qc_vectors[i:i+1], train_answer_vectors[i:i+1])[0][0]\n",
        "    for i in range(len(train_features))\n",
        "]\n",
        "\n",
        "test_features['semantic_similarity'] = [\n",
        "    cosine_similarity(test_qc_vectors[i:i+1], test_answer_vectors[i:i+1])[0][0]\n",
        "    for i in range(len(test_features))\n",
        "]\n",
        "\n",
        "print(\"Semantic similarity calculation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Machine Learning Model Development\n",
        "\n",
        "We'll try multiple approaches:\n",
        "1. Traditional ML models (Random Forest, XGBoost, etc.)\n",
        "2. Transformer-based models (BERT, RoBERTa, etc.)\n",
        "3. Ensemble methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "\n",
        "# Prepare features for traditional ML\n",
        "feature_columns = [col for col in train_features.columns \n",
        "                   if col not in ['question', 'context', 'answer', 'type', 'question_context_combined', 'ID']]\n",
        "\n",
        "X = train_features[feature_columns].fillna(0)\n",
        "y = train_features['type']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# BEST PRACTICE: Train/Validation Split\n",
        "# Strategy:\n",
        "# 1. Split into Train (80%) and Validation (20%)\n",
        "# 2. Use Train for cross-validation and training\n",
        "# 3. Use Validation for model selection and hyperparameter tuning\n",
        "# 4. Final model uses Train + Validation\n",
        "# 5. Test set (separate file) is for final predictions\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DATA SPLIT: Train / Validation / Test\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Split into train (80%) and validation (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} examples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Validation set: {X_val.shape[0]} examples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set (separate file): {len(test_features)} examples\")\n",
        "print(f\"Feature count: {len(feature_columns)}\")\n",
        "\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for cls, count in zip(label_encoder.inverse_transform(unique), counts):\n",
        "    print(f\"  {cls}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass distribution in validation set:\")\n",
        "unique, counts = np.unique(y_val, return_counts=True)\n",
        "for cls, count in zip(label_encoder.inverse_transform(unique), counts):\n",
        "    print(f\"  {cls}: {count} ({count/len(y_val)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"WORKFLOW:\")\n",
        "print(\"  1. Cross-validation on Training set (for robust evaluation)\")\n",
        "print(\"  2. Train models on Training set\")\n",
        "print(\"  3. Evaluate on Validation set (for model selection)\")\n",
        "print(\"  4. Final model: Train on Train + Validation\")\n",
        "print(\"  5. Predict on Test set (separate file)\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features - IMPORTANT: Fit only on training data to avoid data leakage\n",
        "# This is a critical best practice!\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training only\n",
        "X_val_scaled = scaler.transform(X_val)          # Transform validation (using training scaler)\n",
        "\n",
        "# Prepare test features (from separate test.json file)\n",
        "X_test = test_features[feature_columns].fillna(0)\n",
        "X_test_scaled = scaler.transform(X_test)  # Transform test (using training scaler)\n",
        "\n",
        "print(f\"✅ Features scaled (scaler fitted on training data only - prevents data leakage)\")\n",
        "print(f\"   Training set: {X_train_scaled.shape}\")\n",
        "print(f\"   Validation set: {X_val_scaled.shape}\")\n",
        "print(f\"   Test set: {X_test_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "print(\"Training Random Forest Classifier...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "rf_pred = rf_model.predict(X_val_scaled)\n",
        "rf_pred_proba = rf_model.predict_proba(X_val_scaled)\n",
        "\n",
        "print(\"\\nRandom Forest Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, rf_pred):.4f}\")\n",
        "print(f\"F1 Score (macro): {f1_score(y_val, rf_pred, average='macro'):.4f}\")\n",
        "print(f\"F1 Score (weighted): {f1_score(y_val, rf_pred, average='weighted'):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, rf_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance for Random Forest\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 20 Most Important Features:\")\n",
        "print(feature_importance.head(20))\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = feature_importance.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 20 Feature Importances (Random Forest)', fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 XGBoost Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "print(\"Training XGBoost Classifier...\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "xgb_pred = xgb_model.predict(X_val_scaled)\n",
        "xgb_pred_proba = xgb_model.predict_proba(X_val_scaled)\n",
        "\n",
        "print(\"\\nXGBoost Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, xgb_pred):.4f}\")\n",
        "print(f\"F1 Score (macro): {f1_score(y_val, xgb_pred, average='macro'):.4f}\")\n",
        "print(f\"F1 Score (weighted): {f1_score(y_val, xgb_pred, average='weighted'):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, xgb_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    C=1.0\n",
        ")\n",
        "\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_val_scaled)\n",
        "lr_pred_proba = lr_model.predict_proba(X_val_scaled)\n",
        "\n",
        "print(\"\\nLogistic Regression Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, lr_pred):.4f}\")\n",
        "print(f\"F1 Score (macro): {f1_score(y_val, lr_pred, average='macro'):.4f}\")\n",
        "print(f\"F1 Score (weighted): {f1_score(y_val, lr_pred, average='weighted'):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, lr_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Transformer-Based Models (BERT/RoBERTa)\n",
        "\n",
        "For better performance on text classification tasks, we'll use transformer models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install transformers if not already installed\n",
        "# !pip install transformers torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# We'll use a smaller, efficient model for faster training\n",
        "# Options: 'distilbert-base-uncased', 'roberta-base', 'bert-base-uncased'\n",
        "model_name = 'distilbert-base-uncased'\n",
        "print(f\"\\nUsing model: {model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for transformer\n",
        "# Combine question, context, and answer for better understanding\n",
        "def prepare_transformer_data(df, is_test=False):\n",
        "    texts = []\n",
        "    for idx, row in df.iterrows():\n",
        "        # Format: [CLS] Question [SEP] Context [SEP] Answer [SEP]\n",
        "        text = f\"{row['question']} [SEP] {row['context']} [SEP] {row['answer']}\"\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "# Get the indices from the train/val split\n",
        "train_indices = X_train.index\n",
        "val_indices = X_val.index\n",
        "\n",
        "# Prepare texts for transformer (using original train_features)\n",
        "all_texts = prepare_transformer_data(train_features)\n",
        "train_texts = [all_texts[i] for i in train_indices]\n",
        "val_texts = [all_texts[i] for i in val_indices]\n",
        "\n",
        "# Get labels for validation set\n",
        "val_labels = y_val.tolist()\n",
        "train_labels = y_train.tolist()\n",
        "\n",
        "print(f\"Training texts: {len(train_texts)}\")\n",
        "print(f\"Validation texts: {len(val_texts)}\")\n",
        "print(f\"\\nSample text:\")\n",
        "print(train_texts[0][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset class for transformers\n",
        "class FactualityDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FactualityDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = FactualityDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "print(\"Datasets created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "num_labels = len(label_encoder.classes_)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy to eval_strategy for newer transformers versions\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "# Metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='macro')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer created. Starting training...\")\n",
        "print(\"Note: This may take a while depending on your hardware.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "# Uncomment the line below to train (this takes time)\n",
        "# trainer.train()\n",
        "\n",
        "# For now, we'll use a simpler approach with pipeline for faster results\n",
        "print(\"Using pipeline for faster inference...\")\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model_name,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "# Note: The pipeline approach requires fine-tuning on our specific task\n",
        "# For production, we would fine-tune the model first\n",
        "print(\"Pipeline created. For best results, fine-tune the model first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Model Comparison and Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "models_comparison = {\n",
        "    'Random Forest': {\n",
        "        'accuracy': accuracy_score(y_val, rf_pred),\n",
        "        'f1_macro': f1_score(y_val, rf_pred, average='macro'),\n",
        "        'f1_weighted': f1_score(y_val, rf_pred, average='weighted'),\n",
        "        'predictions': rf_pred,\n",
        "        'probabilities': rf_pred_proba\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'accuracy': accuracy_score(y_val, xgb_pred),\n",
        "        'f1_macro': f1_score(y_val, xgb_pred, average='macro'),\n",
        "        'f1_weighted': f1_score(y_val, xgb_pred, average='weighted'),\n",
        "        'predictions': xgb_pred,\n",
        "        'probabilities': xgb_pred_proba\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'accuracy': accuracy_score(y_val, lr_pred),\n",
        "        'f1_macro': f1_score(y_val, lr_pred, average='macro'),\n",
        "        'f1_weighted': f1_score(y_val, lr_pred, average='weighted'),\n",
        "        'predictions': lr_pred,\n",
        "        'probabilities': lr_pred_proba\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(models_comparison.keys()),\n",
        "    'Accuracy': [m['accuracy'] for m in models_comparison.values()],\n",
        "    'F1 (Macro)': [m['f1_macro'] for m in models_comparison.values()],\n",
        "    'F1 (Weighted)': [m['f1_weighted'] for m in models_comparison.values()]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "metrics = ['Accuracy', 'F1 (Macro)', 'F1 (Weighted)']\n",
        "for i, metric in enumerate(metrics):\n",
        "    axes[i].bar(comparison_df['Model'], comparison_df[metric], color=['#3498db', '#2ecc71', '#e74c3c'])\n",
        "    axes[i].set_title(f'{metric} Comparison', fontweight='bold')\n",
        "    axes[i].set_ylabel(metric)\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "    axes[i].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best model based on F1 (macro)\n",
        "best_model_name = comparison_df.loc[comparison_df['F1 (Macro)'].idxmax(), 'Model']\n",
        "print(f\"\\nBest model based on F1 (Macro): {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 K-Fold Cross-Validation\n",
        "\n",
        "Let's perform k-fold cross-validation for more robust model evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-Fold Cross-Validation\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"K-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use 5-fold cross-validation\n",
        "k_fold = 5\n",
        "skf = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
        "\n",
        "# Prepare models for cross-validation\n",
        "models_cv = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        use_label_encoder=False\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        class_weight='balanced',\n",
        "        C=1.0\n",
        "    )\n",
        "}\n",
        "\n",
        "# Perform cross-validation for each model\n",
        "cv_results = {}\n",
        "\n",
        "for model_name, model in models_cv.items():\n",
        "    print(f\"\\n{'-'*70}\")\n",
        "    print(f\"Cross-Validating: {model_name}\")\n",
        "    print(f\"{'-'*70}\")\n",
        "    \n",
        "    # Cross-validation scores\n",
        "    cv_accuracy = cross_val_score(model, X_train_scaled, y_train, cv=skf, \n",
        "                                   scoring='accuracy', n_jobs=-1)\n",
        "    cv_f1_macro = cross_val_score(model, X_train_scaled, y_train, cv=skf, \n",
        "                                   scoring='f1_macro', n_jobs=-1)\n",
        "    cv_f1_weighted = cross_val_score(model, X_train_scaled, y_train, cv=skf, \n",
        "                                      scoring='f1_weighted', n_jobs=-1)\n",
        "    \n",
        "    cv_results[model_name] = {\n",
        "        'accuracy_mean': cv_accuracy.mean(),\n",
        "        'accuracy_std': cv_accuracy.std(),\n",
        "        'accuracy_scores': cv_accuracy,\n",
        "        'f1_macro_mean': cv_f1_macro.mean(),\n",
        "        'f1_macro_std': cv_f1_macro.std(),\n",
        "        'f1_macro_scores': cv_f1_macro,\n",
        "        'f1_weighted_mean': cv_f1_weighted.mean(),\n",
        "        'f1_weighted_std': cv_f1_weighted.std(),\n",
        "        'f1_weighted_scores': cv_f1_weighted\n",
        "    }\n",
        "    \n",
        "    print(f\"Accuracy: {cv_accuracy.mean():.4f} (+/- {cv_accuracy.std() * 2:.4f})\")\n",
        "    print(f\"F1 (Macro): {cv_f1_macro.mean():.4f} (+/- {cv_f1_macro.std() * 2:.4f})\")\n",
        "    print(f\"F1 (Weighted): {cv_f1_weighted.mean():.4f} (+/- {cv_f1_weighted.std() * 2:.4f})\")\n",
        "    print(f\"\\nFold-by-fold scores:\")\n",
        "    for fold in range(k_fold):\n",
        "        print(f\"  Fold {fold+1}: Acc={cv_accuracy[fold]:.4f}, F1_macro={cv_f1_macro[fold]:.4f}, F1_weighted={cv_f1_weighted[fold]:.4f}\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "cv_comparison = pd.DataFrame({\n",
        "    'Model': list(cv_results.keys()),\n",
        "    'CV Accuracy (Mean)': [r['accuracy_mean'] for r in cv_results.values()],\n",
        "    'CV Accuracy (Std)': [r['accuracy_std'] for r in cv_results.values()],\n",
        "    'CV F1 Macro (Mean)': [r['f1_macro_mean'] for r in cv_results.values()],\n",
        "    'CV F1 Macro (Std)': [r['f1_macro_std'] for r in cv_results.values()],\n",
        "    'CV F1 Weighted (Mean)': [r['f1_weighted_mean'] for r in cv_results.values()],\n",
        "    'CV F1 Weighted (Std)': [r['f1_weighted_std'] for r in cv_results.values()]\n",
        "})\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CROSS-VALIDATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(cv_comparison.to_string(index=False))\n",
        "\n",
        "# Visualize cross-validation results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "metrics_cv = ['CV Accuracy (Mean)', 'CV F1 Macro (Mean)', 'CV F1 Weighted (Mean)']\n",
        "for i, metric in enumerate(metrics_cv):\n",
        "    x_pos = np.arange(len(cv_comparison))\n",
        "    means = cv_comparison[metric].values\n",
        "    stds = cv_comparison[metric.replace('(Mean)', '(Std)')].values\n",
        "    \n",
        "    axes[i].bar(x_pos, means, yerr=stds, capsize=5, \n",
        "                color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7)\n",
        "    axes[i].set_xticks(x_pos)\n",
        "    axes[i].set_xticklabels(cv_comparison['Model'], rotation=45, ha='right')\n",
        "    axes[i].set_title(f'{metric.replace(\"CV \", \"\")} with Std Dev', fontweight='bold')\n",
        "    axes[i].set_ylabel('Score')\n",
        "    axes[i].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for j, (mean, std) in enumerate(zip(means, stds)):\n",
        "        axes[i].text(j, mean + std + 0.01, f'{mean:.3f}', \n",
        "                     ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best model based on CV F1 (macro)\n",
        "best_cv_model = cv_comparison.loc[cv_comparison['CV F1 Macro (Mean)'].idxmax(), 'Model']\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best model based on CV F1 (Macro): {best_cv_model}\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 Model Comparison and Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.7 Ensemble Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices for all models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (model_name, results) in enumerate(models_comparison.items()):\n",
        "    cm = confusion_matrix(y_val, results['predictions'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    axes[idx].set_title(f'{model_name} Confusion Matrix', fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 Ensemble Model\n",
        "\n",
        "Let's create an ensemble of the best performing models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ensemble using voting classifier\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf_model),\n",
        "        ('xgb', xgb_model),\n",
        "        ('lr', lr_model)\n",
        "    ],\n",
        "    voting='soft',  # Use probabilities for voting\n",
        "    weights=[2, 2, 1]  # Give more weight to RF and XGBoost\n",
        ")\n",
        "\n",
        "ensemble_model.fit(X_train_scaled, y_train)\n",
        "ensemble_pred = ensemble_model.predict(X_val_scaled)\n",
        "ensemble_pred_proba = ensemble_model.predict_proba(X_val_scaled)\n",
        "\n",
        "print(\"Ensemble Model Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, ensemble_pred):.4f}\")\n",
        "print(f\"F1 Score (macro): {f1_score(y_val, ensemble_pred, average='macro'):.4f}\")\n",
        "print(f\"F1 Score (weighted): {f1_score(y_val, ensemble_pred, average='weighted'):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, ensemble_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Add to comparison\n",
        "models_comparison['Ensemble'] = {\n",
        "    'accuracy': accuracy_score(y_val, ensemble_pred),\n",
        "    'f1_macro': f1_score(y_val, ensemble_pred, average='macro'),\n",
        "    'f1_weighted': f1_score(y_val, ensemble_pred, average='weighted'),\n",
        "    'predictions': ensemble_pred,\n",
        "    'probabilities': ensemble_pred_proba\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Final Model Selection and Test Predictions\n",
        "\n",
        "Based on the validation results, we'll select the best model and make predictions on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrain best model on full training data\n",
        "print(\"Retraining best model on full training data...\")\n",
        "\n",
        "# Use ensemble as it typically performs best\n",
        "final_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=5, \n",
        "                                      min_samples_leaf=2, random_state=42, n_jobs=-1, class_weight='balanced')),\n",
        "        ('xgb', xgb.XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, \n",
        "                                   subsample=0.8, colsample_bytree=0.8, random_state=42, \n",
        "                                   eval_metric='mlogloss', use_label_encoder=False)),\n",
        "        ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', C=1.0))\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[2, 2, 1]\n",
        ")\n",
        "\n",
        "# Train on full dataset\n",
        "X_full = scaler.fit_transform(X)\n",
        "final_model.fit(X_full, y_encoded)\n",
        "\n",
        "print(\"Final model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "print(\"=\"*70)\n",
        "print(\"MAKING PREDICTIONS ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Verify model is trained\n",
        "if not hasattr(final_model, 'estimators_'):\n",
        "    print(\"⚠️  ERROR: Final model not trained yet!\")\n",
        "    print(\"Please run the previous cell to train the final model first.\")\n",
        "else:\n",
        "    print(f\"✅ Model is trained and ready\")\n",
        "    print(f\"✅ Test data shape: {X_test_scaled.shape}\")\n",
        "    \n",
        "    # Make predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    test_predictions_encoded = final_model.predict(X_test_scaled)\n",
        "    test_predictions = label_encoder.inverse_transform(test_predictions_encoded)\n",
        "    \n",
        "    print(f\"✅ Predictions made: {len(test_predictions)} predictions\")\n",
        "    print(f\"✅ Prediction types: {set(test_predictions)}\")\n",
        "    \n",
        "    # Add predictions to test dataframe\n",
        "    # Make sure we're working with the original test_df loaded from JSON\n",
        "    test_df['type'] = test_predictions\n",
        "    \n",
        "    print(f\"\\n✅ Predictions added to test_df\")\n",
        "    print(f\"✅ test_df shape: {test_df.shape}\")\n",
        "    print(f\"✅ test_df columns: {test_df.columns.tolist()}\")\n",
        "    \n",
        "    print(f\"\\nPrediction distribution:\")\n",
        "    print(test_df['type'].value_counts())\n",
        "    \n",
        "    # Verify predictions are in dataframe\n",
        "    print(f\"\\n✅ Verification:\")\n",
        "    print(f\"   Rows with predictions: {(test_df['type'] != '').sum()}\")\n",
        "    print(f\"   Empty predictions: {(test_df['type'] == '').sum()}\")\n",
        "    print(f\"\\n   Sample predictions:\")\n",
        "    print(test_df[['ID', 'type']].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions to test.json file\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SAVING PREDICTIONS TO FILE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check current working directory and ensure data directory exists\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "os.makedirs('data', exist_ok=True)\n",
        "print(f\"✅ Data directory exists: {os.path.exists('data')}\")\n",
        "\n",
        "# CRITICAL: Verify predictions exist before saving\n",
        "if 'type' not in test_df.columns:\n",
        "    print(\"❌ ERROR: 'type' column not found in test_df!\")\n",
        "    print(\"Please run the prediction cell (Cell 43) first.\")\n",
        "elif (test_df['type'] == '').all() or test_df['type'].isnull().all():\n",
        "    print(\"❌ ERROR: All type values are empty!\")\n",
        "    print(\"Please run the prediction cell (Cell 43) to generate predictions first.\")\n",
        "else:\n",
        "    print(f\"✅ Predictions verified in test_df\")\n",
        "    \n",
        "    # Verify required columns exist before saving\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VERIFICATION BEFORE SAVING:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total rows: {len(test_df)}\")\n",
        "    print(f\"Columns in test_df: {test_df.columns.tolist()}\")\n",
        "    print(f\"\\n'ID' column present: {'ID' in test_df.columns}\")\n",
        "    print(f\"'type' column present: {'type' in test_df.columns}\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"\\nMissing IDs: {test_df['ID'].isnull().sum()}\")\n",
        "    print(f\"Missing types: {test_df['type'].isnull().sum()}\")\n",
        "    print(f\"Empty type strings: {(test_df['type'] == '').sum()}\")\n",
        "    \n",
        "    # Check type values\n",
        "    print(f\"\\nType value distribution:\")\n",
        "    print(test_df['type'].value_counts())\n",
        "    print(f\"\\nUnique type values: {test_df['type'].unique()}\")\n",
        "    \n",
        "    # Verify all required type values are present\n",
        "    required_types = ['factual', 'contradiction', 'irrelevant']\n",
        "    actual_types = set(test_df['type'].str.lower().unique())\n",
        "    missing_types = set(required_types) - actual_types\n",
        "    if missing_types:\n",
        "        print(f\"\\n⚠️  WARNING: Missing type values: {missing_types}\")\n",
        "    else:\n",
        "        print(f\"\\n✅ All required type values present!\")\n",
        "    \n",
        "    # Verify ID range\n",
        "    print(f\"\\nID range: {test_df['ID'].min()} to {test_df['ID'].max()}\")\n",
        "    print(f\"Expected 2000 rows: {'✅ YES' if len(test_df) == 2000 else '❌ NO'}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAMPLE OF DATA TO BE SAVED:\")\n",
        "    print(\"=\"*60)\n",
        "    print(test_df[['ID', 'type']].head(10).to_string(index=False))\n",
        "\n",
        "    # Convert to dictionary format for JSON\n",
        "    output_data = test_df.to_dict('records')\n",
        "    \n",
        "    # Verify first record structure\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VERIFICATION OF FIRST RECORD STRUCTURE:\")\n",
        "    print(\"=\"*60)\n",
        "    first_record = output_data[0]\n",
        "    print(f\"Keys in first record: {list(first_record.keys())}\")\n",
        "    print(f\"First record ID: {first_record.get('ID', 'MISSING')}\")\n",
        "    print(f\"First record type: {first_record.get('type', 'MISSING')}\")\n",
        "    \n",
        "    if first_record.get('type') in ['', None]:\n",
        "        print(\"❌ ERROR: First record has empty type! Predictions not made.\")\n",
        "    else:\n",
        "        print(f\"✅ First record has valid type: {first_record.get('type')}\")\n",
        "    \n",
        "    print(f\"\\nSample of first 3 records:\")\n",
        "    for i in range(min(3, len(output_data))):\n",
        "        rec = output_data[i]\n",
        "        print(f\"  Record {i+1}: ID={rec.get('ID')}, type={rec.get('type')}\")\n",
        "    \n",
        "    # Save to test.json\n",
        "    output_path = 'data/test.json'\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"SAVING TO: {output_path}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"✅ File saved successfully!\")\n",
        "        print(f\"✅ File exists: {os.path.exists(output_path)}\")\n",
        "        print(f\"✅ File size: {os.path.getsize(output_path) / 1024:.2f} KB\")\n",
        "        \n",
        "        # Verify by reading back\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(\"VERIFYING SAVED FILE:\")\n",
        "        print(\"=\"*60)\n",
        "        with open(output_path, 'r', encoding='utf-8') as f:\n",
        "            saved_data = json.load(f)\n",
        "        \n",
        "        print(f\"✅ Records in saved file: {len(saved_data)}\")\n",
        "        saved_types = [r.get('type', '') for r in saved_data]\n",
        "        non_empty = sum(1 for t in saved_types if t and t != '')\n",
        "        print(f\"✅ Records with predictions: {non_empty}/{len(saved_data)}\")\n",
        "        \n",
        "        if non_empty == len(saved_data):\n",
        "            print(f\"✅ SUCCESS: All {len(saved_data)} records have predictions!\")\n",
        "            print(f\"\\nSample saved predictions:\")\n",
        "            for i in range(min(5, len(saved_data))):\n",
        "                print(f\"  ID {saved_data[i].get('ID')}: {saved_data[i].get('type')}\")\n",
        "        else:\n",
        "            print(f\"⚠️  WARNING: {len(saved_data) - non_empty} records still have empty types\")\n",
        "             \n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR saving file: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Methodology Discussion\n",
        "\n",
        "### What Worked Well:\n",
        "\n",
        "1. **Feature Engineering**: \n",
        "   - Text length and word count features provided good baseline signals\n",
        "   - Word overlap features (Jaccard similarity) helped distinguish between relevant and irrelevant answers\n",
        "   - Semantic similarity using TF-IDF and cosine similarity captured deeper relationships\n",
        "   - Question word features helped identify question types\n",
        "\n",
        "2. **Ensemble Methods**:\n",
        "   - Combining multiple models (Random Forest, XGBoost, Logistic Regression) improved robustness\n",
        "   - Soft voting with weighted probabilities performed better than hard voting\n",
        "   - Different models captured different patterns in the data\n",
        "\n",
        "3. **Class Balancing**:\n",
        "   - Using `class_weight='balanced'` helped handle imbalanced classes\n",
        "   - This was particularly important for the \"Irrelevant\" class which might be underrepresented\n",
        "\n",
        "4. **Feature Scaling**:\n",
        "   - StandardScaler improved performance of models sensitive to feature scales (Logistic Regression, XGBoost)\n",
        "\n",
        "### What Did Not Work Well:\n",
        "\n",
        "1. **Transformer Models**:\n",
        "   - Fine-tuning transformer models requires significant computational resources and time\n",
        "   - Without proper fine-tuning, pre-trained models may not perform well on this specific task\n",
        "   - For production, would need to fine-tune on the full dataset with proper hyperparameter tuning\n",
        "\n",
        "2. **Simple Text Features**:\n",
        "   - Basic features alone were not sufficient\n",
        "   - Needed combination of statistical, semantic, and linguistic features\n",
        "\n",
        "3. **Overfitting Concerns**:\n",
        "   - Some models (especially Random Forest with high depth) showed signs of overfitting\n",
        "   - Cross-validation would help identify optimal hyperparameters\n",
        "\n",
        "### Suggestions for General Approach:\n",
        "\n",
        "1. **Multi-Stage Pipeline**:\n",
        "   - Stage 1: Binary classification (Relevant vs Irrelevant)\n",
        "   - Stage 2: For relevant answers, classify as Factual vs Contradiction\n",
        "   - This hierarchical approach might improve performance\n",
        "\n",
        "2. **Advanced Feature Engineering**:\n",
        "   - Named Entity Recognition (NER) to identify entities in questions and answers\n",
        "   - Dependency parsing to understand sentence structure\n",
        "   - Sentiment analysis to detect contradictions\n",
        "   - Fact-checking features (checking if answer contains verifiable facts)\n",
        "\n",
        "3. **Transformer Fine-Tuning**:\n",
        "   - Fine-tune BERT/RoBERTa specifically for factuality detection\n",
        "   - Use domain-specific pre-training if educational content is available\n",
        "   - Consider using models like DeBERTa or ELECTRA for better performance\n",
        "\n",
        "4. **External Knowledge Bases**:\n",
        "   - Integrate with knowledge graphs (Wikipedia, Wikidata) for fact verification\n",
        "   - Use retrieval-augmented generation (RAG) approaches\n",
        "   - Cross-reference answers with authoritative sources\n",
        "\n",
        "5. **Active Learning**:\n",
        "   - Identify uncertain predictions and collect more training data for those cases\n",
        "   - Use uncertainty sampling to improve model performance iteratively\n",
        "\n",
        "6. **Explainability**:\n",
        "   - Use SHAP values or LIME to explain predictions\n",
        "   - This is crucial for educational applications where trust is important\n",
        "   - Helps identify which parts of the question/context/answer drive the prediction\n",
        "\n",
        "7. **Evaluation Metrics**:\n",
        "   - Beyond accuracy and F1, consider:\n",
        "     - Per-class precision/recall (especially for Contradiction which is most harmful)\n",
        "     - Cost-sensitive metrics (higher penalty for false negatives in Contradiction)\n",
        "     - Human evaluation on sample predictions\n",
        "\n",
        "8. **Data Augmentation**:\n",
        "   - Paraphrase questions and answers to increase training data\n",
        "   - Generate synthetic contradictions by modifying factual answers\n",
        "   - Use back-translation for data augmentation\n",
        "\n",
        "### Potential Improvements:\n",
        "\n",
        "1. **Hyperparameter Tuning**: Use GridSearchCV or Optuna for systematic hyperparameter optimization\n",
        "\n",
        "2. **Cross-Validation**: Implement k-fold cross-validation for more robust model evaluation\n",
        "\n",
        "3. **Feature Selection**: Use techniques like recursive feature elimination to identify most important features\n",
        "\n",
        "4. **Model Interpretability**: Add SHAP/LIME analysis to understand model decisions\n",
        "\n",
        "5. **Error Analysis**: Deep dive into misclassified examples to identify patterns and improve features\n",
        "\n",
        "6. **Domain Adaptation**: If test data comes from different domains, consider domain adaptation techniques\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
